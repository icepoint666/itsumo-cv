1.	首先，多物体跟踪一个简短的综述
-	多物体跟踪multi-object tracking,主要解决的问题是：
有一段视频，是由N个连续帧组成，从第一帧到最后一帧，里面会有很多个目标，有进有出，不断的运动。
-	我们的目的：
就是对每个目标，能够跟其他目标区分开，能够跟踪它在不同帧中的轨迹，
得到的结果就是，每帧中跟踪到每个物体的编号（人为定的）还有这个物体的边框，（就像这张slice上面的图一样），就是这样一个过程。
-	那么他的实现流程，大多数都是这样：
1）	首先，用object detection目标检测把各帧中需要追踪的目标都检测出来，检测这一类物体。
2）	先初始化物体的编号，之后开始，从前到后，通过上一帧以及前面一些帧的信息，来预测下一帧以及后面帧追踪物体的位置，这些信息主要外观特征，运动特征，碰撞模型，等还有其他的特征来估计
（（假如一个人穿衣服的颜色是红色，。。。。等光学信息，还有普遍的匀速运动模型来计算出这个物体最大概率出现的bounding box区域））
3）	把你预测的下一帧目标的位置，与实际下一帧目标的位置，进行配比，通过data association数据关联的方式，类似这样的图匹配
（这个也是多目标跟踪相比于普通单目标跟踪额外的一个工作步骤）
-	当然也不一定只有这种思路，因为这种做法弊端就是太依赖物体检测的效果，只是用的最多的一种思路
-	现在这个方向主要研究工作是这些：
-	外观模型，运动模型，交互模型，
-	遮挡处理，数据关联
-	近几年突破口是：
-	我了解的几篇论文主要突破口都是：
1）用深度学习设计一种端到端的神经网络，主要突破是设计一种比较好的loss函数
2）除了这种应该就是相关滤波方法上突破
-	目前还没有解决好的一些难点主要是：
-	Challenges: Occlusions,similar appearance,complex motion,false alarms.这些问题基本上是每篇论文都在努力解决的。
-	Occlusions：有三种，被场景中的物体遮挡，被其他 target 遮挡，被自己遮挡（如变形，无法检测到），遮挡之后，本来应该检测到的 target 就检测不到了。解决的方法目前是根据 temporal 时序信息，估计出某一帧的某个位置有 target 被遮挡了。
-	Similar Appearance：一是怎么把 target 与背景分开；二是怎么把不同的 target 分开。一般需要设计一个很好的外观模型，比如用 HOG、color histogram 等等。
-	Complex Motion: 最简单的情况是匀速直线运动，这样我们很好预测下一帧中这个 target 在哪，但实际情况往往并不如此。比如可能来了个急转弯，可能突然转身往回走。解决之道，一般要设计更灵活、更复杂的运动模型。
-	False Alarms: detector 给出了 response，但实际上那个地方并没有 target，误检。也就是增强物体检测的效果，这就需要可信度来鉴别。
(基本上就这样，整体写的比较简单)
2.	这篇论文：
-	它本身其实是一个MOT-challenge一个官方多目标追踪竞赛的一个提交结果，很多效果很好的想法，测试效果很好，都发论文了，所以他们用的数据集都是同一个官方的数据集。
-	这篇论文主要处理的问题是：
现阶段使用deep learning处理multi-object tracking问题的场景下：
存在这些问题
1）.	对于多目标跟踪的数据，是不够去训练一个有大数量参数的深度神经网络 
（因为竞赛都是固定的数据集，最权威的MOT数据集不是很大，所以深度学习方法进行有很大限制）
2）．对于存在的深度学习的网络在分类有很微小差异的object时，存在很大限制
3）. 深度学习比较善于捕获图像特征，但捕获视频中运动特征时比较困难
主要解决的问题也就是：
如何用深度学习来更好的进行multi-object-tracking,达到好的效果
	提出的主要方法是：
四元CNN网络
-	因为是这样，对于多目标跟踪，他的目的把跨帧检测到的detections关联起来，所以关键是要设计出来一个衡量detections之间的相似度函数，作为度量，来进行关联，文章提出的就是公式（4）,其实就是分为两部分，前面是图像提取的外观特征，后面是提取的运动特征。
（这里就解决了一个问题：它即捕获外观特征又捕获运动特征）
为什么叫四元，本身是一种度量学习的方法，四元损失函数，是由Siamese network 还有triplet network的提升版，主要用于人脸识别和物体追踪，物体检测，相似度比较小的物体，所以就又能很好应对前面的问题

-	四元其实网络的输入其实是跨帧的4个detections
-	本身先取一个detection作为ancher sample，
-	之后取后面时间t1,t2发生的两个和这个是相同物体的作为正样本，再在视频的帧中取不同物体作为负样本，
-	（它这些detection组合，数据量也会有一个比较大，可以应对前面数据量小的问题）
-	损失函数就是鼓励网络，在相同物体之间更小的距离，不同物体之间有更大的度量距离（这个损失函数的意义）。 
这是它最核心的思路：
整个网络结构比较复杂：
是一个端到端的神经网络
输入就是一个mini-batch里面每个是四元detections就是一个输入，还有序列统计量
损失函数总结起来说其实就是一个多任务损失函数，
1）	前面一部分是前面提到的用于度量相似度的损失函数，2）另一部分是因为数据关联依赖于一个精确的物体定位，所以利用bounding box regression,作为一个额外目标学习
整个损失函数都是通过输入通过一层一层网络得到中间特征向量
（圆圈1,2,3就是通过网络计算得到的值）
圆圈1：就是外观特征通过单一detection 经过cnn网络提取特征得到，
圆圈2：运动特征，通过两个detections之间的关系，得到
圆圈3：就是这两个特征比例的参数，，通过时间间隔经过全连接层计算得到
红色长方体表示的都是全连接层，所以就是训练这些全连接层，以及CNN网络的参数
训练完之后，每次输入两个任意帧的两个detection，结合视频序列信息统计量的输入，经过网络计算就可以得到这两个detection之间的d距离，也就是用来衡量相似度。
这就是数据关联用到的，

数据关联算法：
就是把多目标跟踪传播过程当成一个有向图
节点就是detection，边的cost就是相似度d，
形成边的条件就是当cost小于一定阈值，就认定是差不多同一object进行连接
从初始的帧一直传播先去，（当然传播不只是相邻帧之间的detection进行传播，是一个跨帧传播）直到没有可以相连的边，再从前往后到下一个新的物体开始

