# Alexnet
## Alexnet结构
![ ](./pics/alexnet1.jpg  "alexnet_structure")

## Alexnet简介
alexnet总共包括8层，其中前5层convolutional，后面3层是full-connected，文章里面说的是减少任何一个卷积结果会变得很差，下面我来具体讲讲每一层的构成：

### 第一层
输入图像为227\*227\*3(paper上貌似有点问题224\*224\*3)的图像

使用了96个kernels（96,11,11,3)

卷积核进行对原始图像的每次卷积都生成一个新的像素。

卷积核沿原始图像的x轴方向和y轴方向两个方向移动，移动的步长是4个像素。

因此，卷积核在移动的过程中会生成 (227-11)/4+1=55 个像素(227个像素减去11，正好是54，即生成54个像素，再加上被减去的11也对应生成一个像素)，行和列的55\*55个像素形成对原始图像卷积之后的像素层。

共有96个卷积核，会生成55\*55\*96个卷积后的像素层。

96个卷积核分成2组，每组48个卷积核。对应生成2组55\*55\*48的卷积后的像素层数据。

这些像素层经过relu1单元的处理，生成激活像素层，尺寸仍为2组55\*55\*48的像素层数据。

经过response-normalized层（其实是Local Response Normalized，后面会讲下这里）正则化。

这些像素层经过pool运算(池化运算)的处理，池化运算的尺度为3*3，运算的步长为2，则池化后图像的尺寸为(55-3)/2+1=27。 即池化后像素的规模为27*27*96，pool这一层好像caffe里面的alexnet和paper里面不太一样，alexnet里面采样了两个GPU，所以从图上面看第一层卷积层厚度有两部分，池化pool_size=(3,3),滑动步长为2个pixels，得到96个27\*27个feature。

第一卷积层运算结束后形成的像素层的规模为27*27*96。分别对应96个卷积核所运算形成。这96层像素层分为2组，每组48个像素层，每组在一个独立的GPU上进行运算。

### 第二层
第二层输入数据为第一层输出的27*27*96的像素层，为便于后续处理，每幅像素层的左右两边和上下两边都要填充2个像素；

27*27*96的像素数据分成27*27*48的两组像素数据，两组数据分别再两个不同的GPU中进行运算。

每组像素数据被5*5*48的卷积核进行卷积运算，卷积核对每组数据的每次卷积都生成一个新的像素。

卷积核沿原始图像的x轴方向和y轴方向两个方向移动，移动的步长是1个像素。

因此，卷积核在移动的过程中会生成(27-5+2*2)/1+1=27个像素。(27个像素减去5，正好是22，在加上上下、左右各填充的2个像素，即生成26个像素，再加上被减去的5也对应生成一个像素)，行和列的27*27个像素形成对原始图像卷积之后的像素层。

共有256个5*5*48卷积核；这256个卷积核分成两组，每组针对一个GPU中的27*27*48的像素进行卷积运算。会生成两组27*27*128个卷积后的像素层。

这些像素层经过relu2单元的处理，生成激活像素层，尺寸仍为两组27*27*128的像素层。
###
做LRN处理，然后pooled，这些像素层经过pool运算(池化运算)的处理，池化运算的尺度为3*3，运算的步长为2，则池化后图像的尺寸为(27-3)/2+1=13。 即池化后像素的规模为2组13*13*128的像素层；

### 第三层
第三层、第四层都没有LRN和pool，第五层只有pool

第三层输入数据为第二层输出的2组13*13*128的像素层；

为便于后续处理，每幅像素层的左右两边和上下两边都要填充1个像素；

2组像素层数据都被送至2个不同的GPU中进行运算。每个GPU中都有192个卷积核，每个卷积核的尺寸是3*3*256。

因此，每个GPU中的卷积核都能对2组13*13*128的像素层的所有数据进行卷积运算。

卷积核对每组数据的每次卷积都生成一个新的像素。卷积核沿像素层数据的x轴方向和y轴方向两个方向移动，移动的步长是1个像素。

因此，运算后的卷积核的尺寸为(13-3+1*2)/1+1=13（13个像素减去3，正好是10，在加上上下、左右各填充的1个像素，即生成12个像素，再加上被减去的3也对应生成一个像素），每个GPU中共13*13*192个卷积核。

2个GPU中共13*13*384个卷积后的像素层。

这些像素层经过relu3单元的处理，生成激活像素层，尺寸仍为2组13*13*192像素层，共13*13*384个像素层。
### 第四层



全连接层： 前两层分别有4096个神经元，最后输出softmax为1000个（ImageNet），注意caffe图中全连接层中有relu、dropout、innerProduct。

（感谢AnaZou指出上面之前的一些问题） paper里面也指出了这张图是在两个GPU下做的，其中和caffe里面的alexnet可能还真有点差异，但这可能不是重点，各位在使用的时候，直接参考caffe中的alexnet的网络结果，每一层都十分详细，基本的结构理解和上面是一致的。
